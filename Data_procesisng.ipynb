{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697dab25-16b5-4db0-9b9d-4112810d339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 19:54:36.966800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cleantext import clean\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import joblib \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce8866-f9f2-4835-a38c-45fffff145ca",
   "metadata": {},
   "source": [
    "### Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea11e25c-40c7-4a0c-951c-e18b73630366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_cleaned_2018_02_13.csv\", nrows = 50000, skiprows=lambda x: x in range(1, 400000),usecols=[\"type\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb01455-b960-4502-abdf-289bcd4f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(df:pd.DataFrame) -> (pd.DataFrame, str):\n",
    "    \"\"\"\n",
    "        Takes a TextFileReader with dataframes, and cleans the content. \n",
    "        \n",
    "        Returns the new TextFileReader with cleaned content, and a concatenated string of all the contents. \n",
    "    \"\"\"\n",
    "    concat_text = \"\"\n",
    "    i = 0\n",
    "    for text in df[\"content\"]:\n",
    "        if type(text) == str:\n",
    "            date_reg = re.compile(\"\\d{0,4}-\\d{0,2}-\\d{0,2}[^,]+\") # Cleaning the Dates of the text\n",
    "            date_subbed = date_reg.sub(\"<DAT>\", text)\n",
    "            date_reg2 = re.compile(\"/^(?:\\d{4})-(?:\\d{2})-(?:\\d{2})T(?:\\d{2}):(?:\\d{2}):(?:\\d{2}(?:\\.\\d*)?)(?:(?:-(?:\\d{2}):(?:\\d{2})|Z)?)$/\")\n",
    "            date_subbed2 = date_reg2.sub(\"<DAT>\", date_subbed)\n",
    "            cleaned_news_file = clean(date_subbed2,no_line_breaks=True, # Cleaning the rest of the text.\n",
    "                    no_urls=True,                  \n",
    "                    no_emails=True,                   \n",
    "                    no_numbers=True,\n",
    "                    no_punct=False,\n",
    "                    replace_with_number = \"<NUM>\")\n",
    "            df.loc[i,\"content\"] = cleaned_news_file # returning the cleaned text to the right position in our dataframe\n",
    "            \n",
    "            concat_text = concat_text + cleaned_news_file # Making a string with all the content.\n",
    "        else:\n",
    "            pass \n",
    "        i += 1\n",
    "    return df, concat_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14003fd6-c672-4cff-92af-8f870fc9acc1",
   "metadata": {},
   "source": [
    "### Cleaning the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8a7e15-7f7b-477e-a3b5-1ed63d5e2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, news_file  = clean_file(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eac41-271d-4d42-8c21-9de03d62fcf5",
   "metadata": {},
   "source": [
    "### Dropping the articles with types of \"unknown\" or NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57f3d61-ec42-48e5-ae12-54e5eeb59fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the \"unknown\" and nan types. \n",
    "\n",
    "a = ['rumor', 'hate', 'unreliable', 'conspiracy', 'clickbait', 'satire',\n",
    "       'fake', 'reliable', 'bias', 'political', 'junksci']\n",
    "\n",
    "\n",
    "\n",
    "df = df[df[\"type\"].isin(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6878e-a0b2-4645-bedf-d210aef5738b",
   "metadata": {},
   "source": [
    "### Dropping duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f7c880-1b90-4a39-9595-f082276d3c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Dropping duplicate columns\n",
    "     \n",
    "df.drop_duplicates(\"content\",inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ead6bb-6e42-4489-8f10-6a0c3d541189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 26737 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   type     26737 non-null  object\n",
      " 1   content  26737 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 626.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b9e305-7714-4030-802f-798e5cba4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes a string of text and tokenizes it. \n",
    "    \n",
    "    Returns a list of the tokenized text. \n",
    "    \n",
    "    \"\"\"\n",
    "    token_list = text_to_word_sequence(text)\n",
    "    words = [word for word in token_list if word.isalpha()]\n",
    "    return words \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f295cc98-fd58-4aea-9592-413a91413396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stopwords(token_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Takes a list of words and removes the stopwords \n",
    "    \n",
    "    Returns a list of words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in token_list if not w in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b02404cb-ad67-4884-8208-b18f479ea9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(token_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Takes a list of words and stems the words\n",
    "    \n",
    "    Returns a list of stemmed words. \n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in token_list]\n",
    "    return stemmed \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4833b6a4-3946-4df3-b87d-319c47b28dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    \"\"\"\n",
    "    Takes a list and drop all duplicates in the list\n",
    "    \n",
    "    Returns a list with no duplicates\n",
    "    \"\"\"\n",
    "    unique_list = pd.Series(list1).drop_duplicates().tolist()\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec328ff-0a25-4005-8fd8-69e54f0d84b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inserting labels true or false for the article dependent on their types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45279453-4cb5-4beb-a3d0-b52ac167f109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Making the labels for the articles. \n",
    "\n",
    "df_labels = df[\"type\"].isin([\"political\", \"reliable\" , \"clickbait\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceffcbbf-f87b-401a-9b60-63523604a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting the labels in the original Dataframe.\n",
    "\n",
    "df.insert(loc = len(df.columns) , column = \"label\" ,value = df_labels)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c082d9-7e90-4f88-8448-6c85a003ffa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing stopwords and stemming of the content in each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf06e2b-c365-4581-a866-610d7c2e9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming and removing stopwords of the documents. \n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(tokenize)\n",
    "df[\"content\"] = df[\"content\"].apply(removing_stopwords)\n",
    "df[\"content\"] = df[\"content\"].apply(stemming)\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608d17a-fd72-4771-a8d9-f98b9e78853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2 Feature enginering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5e504-32d2-408e-a9f3-c7b767c02dcf",
   "metadata": {},
   "source": [
    "#### Splitting the data into training, validation and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe42ed6-46ea-45de-8d69-9277bb3f7e10",
   "metadata": {},
   "source": [
    "First we want to figure out which labels we want to give to our articles. Using our dataset and the README documentation of the FakeNewsCorp Dataset we see that we have 11 (13 with nan and unknown) different types of articles.\n",
    "\n",
    "We want to transform this into labels of either fake or reliable.\n",
    "Fake and Satire types are obviosly fake, and reliable and political are as reliable as it gets with news. \n",
    "\n",
    "Now to label the rest is a bit more subjective. We have chosen to label clickbait as reliable, as the content itself is probably fine, but the headlines are the questional part. \n",
    "\n",
    "Conspiracy, bias and junksci kinda falls under the same category of not necessarily being fake, but is in the extreme end of opinion to the point most of it probably is fake. \n",
    "Although it is good to note that a lot of modern science was also \"fake\", before it became science. See for example germs. And just straight up dismissing everything that isn't established \"truth\" can also be dangerous. With that said we chose to label these three types as fake. \n",
    "\n",
    "Unreliable as the name suggests is unreliable. From the description on the README, it might be true but it needs more verification. Here it would be nice to have more classes than just reliable and fake so something that are very much in the middle could have a class to go to. \n",
    "For simplecity reasons we gave unreliable the label fake. \n",
    "\n",
    "The hate type is very difficult to place, because it's again not necessarily fake news, but at the same time it smells of extremist propaganda, and that tends to be more fake than true, or at least distorted in such a way that the \"meat\" isnt wrong per say but the conclusion is false. \n",
    "In the end classifying such news as fake is probably better from an ethical point of view, so we dont give legitimacy to rasicm, hate and other forms of discrimination. \n",
    "\n",
    "We are left with rumor, nan and unknown. \n",
    "Rumor is not described in the README, but since its a rumor it should probably be treated with caution until the rumor is confirmed either true or false. So we will label this as fake. \n",
    "\n",
    "We have removed nan and unknown from the dataset, since it doesnt really make sense to label it. We could have also just labeled it fake, and since its a very small percentage of the dataset the difference is probably not that big. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab925e-c4c7-4f33-956c-6bc1f6c59866",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0dbdc-85f8-4fc5-ab13-e9195121ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp, X_val, y_train_temp, y_val  = train_test_split(df, df_labels, test_size = 0.1, random_state=0) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_temp, y_train_temp, test_size = 1/9, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf641f-3055-4411-81ae-b1f5b4535593",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Making the bag of words matrix. We are using the 1000 most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839759f8-1b00-48df-89bc-ce3ff3d30bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer(min_df=5)\n",
    "\n",
    "bow_vectorizer.fit(df[\"content\"])\n",
    "X_train_bow = bow_vectorizer.transform(X_train[\"content\"])\n",
    "X_val_bow = bow_vectorizer.transform(X_val[\"content\"])\n",
    "X_test_bow = bow_vectorizer.transform(X_test[\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2b57e-8f6f-434f-8d5d-03837d6c581d",
   "metadata": {},
   "source": [
    "### Making a TF-IDF Unigram feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c6836-c577-4783-807f-6a52730914c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "unigram_vectorizer.fit(df[\"content\"])\n",
    "\n",
    "X_train_tfidf = unigram_vectorizer.transform(X_train[\"content\"])\n",
    "X_val_tfidf = unigram_vectorizer.transform(X_val[\"content\"])\n",
    "X_test_tfidf = unigram_vectorizer.transform(X_test[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbee17-04db-4a76-b9df-5bd04cc44042",
   "metadata": {},
   "source": [
    "# Making a simple baseline model\n",
    "\n",
    "We start by making a logistic regression model using BoW as features, and another model using unigram of tf_idf as our features.\n",
    "Logistic regression is good for binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367577b5-4a70-4ea1-b07c-4c77c6495e01",
   "metadata": {},
   "source": [
    "### Bag of Words logistric regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c40705c-ae4a-4d2f-8e12-13129c95a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_lreg = LogisticRegression(random_state=0, max_iter=300)\n",
    "bow_lreg.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edeb3f-dd39-4ca4-ae3b-596966882b8f",
   "metadata": {},
   "source": [
    "### Tuning the hyper parameters with the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0c64f-c77e-4aea-ba4a-654f04e194f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_bow_pred = bow_lreg.predict_proba(X_val_bow) # predicting on the validation set\n",
    "lreg_bow_pred_int = lreg_bow_pred[:,1] >= 0.5 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
    "\n",
    "\n",
    "print(f\"Accuracy score: {accuracy_score(y_val, lreg_bow_pred_int)} \\n f1_score:{f1_score(y_val, lreg_bow_pred_int)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593e4fb-400a-42bd-b5b9-41be29da41da",
   "metadata": {},
   "source": [
    "### Unigram TF-IDF logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a989c6a-8f8c-4426-adaa-026a86548c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_lreg = LogisticRegression(random_state=0)\n",
    "tf_lreg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8ad6c-d904-4370-af88-dc8b684030b0",
   "metadata": {},
   "source": [
    "### Using the validation set to tune hyper parameters and feature parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b6008-78d0-409c-ba0e-eb96f061791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_pred = tf_lreg.predict_proba(X_val_tfidf)\n",
    "unigram_pred_int = unigram_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Accuracy score: {accuracy_score(y_val, unigram_pred_int)} \\n f1_score:{f1_score(y_val, unigram_pred_int)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bf3ac-25ef-4dcf-ad0a-9a682b50e584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb99a4c7-0b96-4cd7-830b-04e131229aef",
   "metadata": {},
   "source": [
    "# Part 3 Making a more advanced model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89609544-6b86-415b-8fac-dacd497264d3",
   "metadata": {},
   "source": [
    "### We start by making more advanced features and use the TF-IDF bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1ef40-08c0-4ebb-a331-32555cf38a39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_vect_ngram.fit(df[\"content\"])\n",
    "\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train[\"content\"])\n",
    "X_val_tfidf_ngram =  tfidf_vect_ngram.transform(X_val[\"content\"])\n",
    "X_test_tfidf_ngram =  tfidf_vect_ngram.transform(X_test[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a14409-2f27-4ff7-9632-15ad02e4fd26",
   "metadata": {},
   "source": [
    "### Making naive bayes models.\n",
    "\n",
    "We now try to make naive bayes models with our different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21516cec-4b01-442b-b505-d6244f425209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bigram_naive_MN = MultinomialNB()\n",
    "tfidf_bigram_naive_MN.fit(X_train_tfidf_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f53fc-4db6-4231-a404-4cd550122dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_mn_pred = tfidf_bigram_naive_MN.predict_proba(X_val_tfidf_ngram)\n",
    "naive_mn_pred_int = naive_mn_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Accuracy score: {accuracy_score(y_val, naive_mn_pred_int)} \\n f1_score:{f1_score(y_val, naive_mn_pred_int)}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba6432-cf8f-4aa2-b0ce-5d804f30c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_bigram_naive_C = ComplementNB()\n",
    "tfidf_bigram_naive_C.fit(X_train_tfidf_ngram, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3530c1f-88ba-413e-aafa-61274d830bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_c_pred = tfidf_bigram_naive_C.predict_proba(X_val_tfidf_ngram)\n",
    "naive_c_pred_int = naive_c_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Accuracy score: {accuracy_score(y_val, naive_c_pred_int)} \\n f1_score:{f1_score(y_val, naive_c_pred_int)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134abfd-9016-440d-bef3-954821e89088",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 4 Evaluating performance on the test set, and trying our models on the Liar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ff43f-a851-485e-a57d-b28e716dff58",
   "metadata": {},
   "source": [
    "#### First we try evaluate our models on our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc1f7e-f65d-47d1-996d-1832759b1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_bow_pred = bow_lreg.predict_proba(X_test_bow) \n",
    "lreg_bow_pred_int = lreg_bow_pred[:,1] >= 0.5 # Setting a threshold of 0.5\n",
    "\n",
    "print(f\"BoW Log \\n Accuracy score: {accuracy_score(y_test, lreg_bow_pred_int)} \\n f1_score:{f1_score(y_test, lreg_bow_pred_int)}\")\n",
    "\n",
    "unigram_pred = tf_lreg.predict_proba(X_test_tfidf)\n",
    "unigram_pred_int = unigram_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Unigram Log \\n Accuracy score: {accuracy_score(y_test, unigram_pred_int)} \\n f1_score:{f1_score(y_test, unigram_pred_int)}\")\n",
    "\n",
    "naive_mn_pred = tfidf_bigram_naive_MN.predict_proba(X_test_tfidf_ngram)\n",
    "naive_mn_pred_int = naive_mn_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Multinomial \\n Accuracy score: {accuracy_score(y_test, naive_mn_pred_int)} \\n f1_score:{f1_score(y_test, naive_mn_pred_int)}\")    \n",
    "\n",
    "\n",
    "naive_c_pred = tfidf_bigram_naive_C.predict_proba(X_test_tfidf_ngram)\n",
    "naive_c_pred_int = naive_c_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Complement \\n Accuracy score: {accuracy_score(y_test, naive_c_pred_int)} \\n f1_score:{f1_score(y_test, naive_c_pred_int)}\")     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af3c25-3495-4522-9550-53f120ec7ecf",
   "metadata": {},
   "source": [
    "### Then we make the confusion matrix for the different predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89147484-e13c-478f-8ce6-8d3f5f313e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "predictions = {\"Bow\":lreg_bow_pred_int,\"Unigram\":unigram_pred_int, \"Naive_mn\":naive_mn_pred_int, \"Niave_c\":naive_c_pred_int}\n",
    "    \n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    \n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], [\"Pred N\", \"Pred P\"])\n",
    "    plt.yticks([0, 1], [\"N\", \"P\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"true Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17ec33-a740-4ba0-8de2-11d617315cf0",
   "metadata": {},
   "source": [
    "### Now we try our models on the Liar Data set\n",
    "First we read and clean the Liar data set \n",
    "\n",
    "We have decided to use the train data set because it is the biggest dataset and since we dont have to actually train a new model we just want as much data as possible to test on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c458d-fe0d-4ffc-af2a-1be358fa321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_dataset = pd.read_table(\"liar_dataset/train.tsv\", names=[\"ID\", \"label\",\"content\",\"subjects\", \"speaker\", \"speaker job title\", \"state info\", \"party affli\", \"barely true counts\", \"false counts\", \"half true counts\", \"mostly true counts\", \"pants on fire counts\", \"location\"]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc6a35-7481-43b3-82f8-7bb225c9fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld, liar_news_file = clean_file(liar_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad8001-685c-45f0-88a0-17c04dbe0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld[\"content\"] = ld[\"content\"].apply(tokenize)\n",
    "ld[\"content\"] = ld[\"content\"].apply(removing_stopwords)\n",
    "ld[\"content\"] = ld[\"content\"].apply(stemming)\n",
    "\n",
    "ld[\"content\"] = ld[\"content\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdec672-3ff0-4ecf-938b-39b3fb0b6bad",
   "metadata": {},
   "source": [
    "### Then we label the classes. \n",
    "Here we have chosen to represent half-true, mostly-true and true as the true articles and the rest as false articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fc093-cd5d-48ed-b24c-d016dd79204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list = [\"half-true\", \"mostly-true\", \"true\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf74ce9-3909-4603-b9ea-c6d603a87905",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33327c-3378-40c1-80ef-56a003dcb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforming the labels\n",
    "ld[\"label\"] = ld[\"label\"].isin(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de950e5d-d87a-4196-b7f0-fd8e34e17552",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ld = ld[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e18f-25d5-4e59-9b0f-1fb65981f3f6",
   "metadata": {},
   "source": [
    "### We then make the different feature sets for the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ecede-f976-4aeb-9bb1-f5ce2c064bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_bow = bow_vectorizer.transform(ld[\"content\"])\n",
    "ld_unigram = unigram_vectorizer.transform(ld[\"content\"])\n",
    "ld_ngram = tfidf_vect_ngram.transform(ld[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465e53-1202-41df-8d4a-55032fc1e236",
   "metadata": {},
   "source": [
    "### Now we measure our performance of our models on the Liar dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0816a-9a0d-4a27-82d8-9dae27791c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_bow_pred = bow_lreg.predict_proba(ld_bow) \n",
    "ld_bow_pred_int = ld_bow_pred[:,1] >= 0.5 # Setting a threshold of 0.5\n",
    "\n",
    "print(f\"BoW Log \\n Accuracy score: {accuracy_score(y_ld, ld_bow_pred_int)} \\n f1_score:{f1_score(y_ld, ld_bow_pred_int)}\")\n",
    "\n",
    "ld_unigram_pred = tf_lreg.predict_proba(ld_unigram)\n",
    "ld_unigram_pred_int = ld_unigram_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Unigram Log \\n Accuracy score: {accuracy_score(y_ld, ld_unigram_pred_int)} \\n f1_score:{f1_score(y_ld, ld_unigram_pred_int)}\")\n",
    "\n",
    "ld_naive_mn_pred = tfidf_bigram_naive_MN.predict_proba(ld_ngram)\n",
    "ld_naive_mn_pred_int = ld_naive_mn_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Multinomial \\n Accuracy score: {accuracy_score(y_ld, ld_naive_mn_pred_int)} \\n f1_score:{f1_score(y_ld, ld_naive_mn_pred_int)}\")    \n",
    "\n",
    "\n",
    "ld_naive_c_pred = tfidf_bigram_naive_C.predict_proba(ld_ngram)\n",
    "ld_naive_c_pred_int = ld_naive_c_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Complement \\n Accuracy score: {accuracy_score(y_ld, ld_naive_c_pred_int)} \\n f1_score:{f1_score(y_ld, ld_naive_c_pred_int)}\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087d2ab-7fee-4f3a-8f86-58e5a4f8df63",
   "metadata": {},
   "source": [
    "### Then we make the confusion matrix for the ld dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd06f8-3160-4363-86e0-1a08b55a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "ld_predictions = {\"Bow\":ld_bow_pred_int,\"Unigram\":ld_unigram_pred_int, \"Naive_mn\":ld_naive_mn_pred_int, \"Niave_c\":ld_naive_c_pred_int}\n",
    "    \n",
    "for i, (name, pred) in enumerate(ld_predictions.items()):\n",
    "    cm = confusion_matrix(y_ld, pred)\n",
    "    \n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], [\"Pred N\", \"Pred P\"])\n",
    "    plt.yticks([0, 1], [\"N\", \"P\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"true Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b72a76-25ae-49cb-833d-b470f7fdc728",
   "metadata": {},
   "source": [
    "We can see that our models are terrible and it mainly just thinks that every article is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c54e4b-81e9-4544-a36c-7a189038b73e",
   "metadata": {},
   "source": [
    "### Trying out the model on completely different data from the data set.\n",
    "\n",
    "### We start by preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c81e6-0271-45ab-b0d1-003b3b138d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"news_cleaned_2018_02_13.csv\", nrows=10000, skiprows=lambda x: x in range(1, 500000),usecols=[\"type\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ad27c-9eba-417c-aec2-e6e4027ef4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, _ = clean_file(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d562b-bfb9-4dbd-98e6-2d49eeac4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['rumor', 'hate', 'unreliable', 'conspiracy', 'clickbait', 'satire',\n",
    "       'fake', 'reliable', 'bias', 'political', 'junksci']\n",
    "\n",
    "df1 = df1[df1[\"type\"].isin(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca630a3c-cf02-4002-8941-01edfea85060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop_duplicates(\"content\",inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b612754-03db-405e-b7ba-4df05c041eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_labels = df1[\"type\"].isin([\"political\", \"reliable\" , \"clickbait\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313537c-b7f3-4b50-ae09-1efd422d3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.insert(loc = len(df1.columns) , column = \"label\" ,value = df1_labels)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18227463-0366-4dd4-905e-32ca7dd79070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"content\"] = df1[\"content\"].apply(tokenize)\n",
    "df1[\"content\"] = df1[\"content\"].apply(removing_stopwords)\n",
    "df1[\"content\"] = df1[\"content\"].apply(stemming)\n",
    "df1[\"content\"] = df1[\"content\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f77bb-ffb7-45b0-8d3d-fbc4a94e368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = df1[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b1f7e-c4f9-42c5-ada1-daf35d6e347a",
   "metadata": {},
   "source": [
    "## Then we make the feature sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0af18b-ec4b-4c3a-92aa-e1728e99e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_bow = bow_vectorizer.transform(df1[\"content\"])\n",
    "df1_unigram = unigram_vectorizer.transform(df1[\"content\"])\n",
    "df1_ngram = tfidf_vect_ngram.transform(df1[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2fa7d-6be3-4e9d-839d-669448c7a198",
   "metadata": {},
   "source": [
    "## We test the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49b917-fbab-4cd8-bb47-3cf99af47b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_lreg_bow_pred = bow_lreg.predict_proba(df1_bow) \n",
    "new_lreg_bow_pred_int = new_lreg_bow_pred[:,1] >= 0.5 # Setting a threshold of 0.5\n",
    "\n",
    "print(f\"BoW Log \\n Accuracy score: {accuracy_score(new_test, new_lreg_bow_pred_int)} \\n f1_score:{f1_score(new_test, new_lreg_bow_pred_int)}\")\n",
    "\n",
    "new_unigram_pred = tf_lreg.predict_proba(df1_unigram)\n",
    "new_unigram_pred_int = new_unigram_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Unigram Log \\n Accuracy score: {accuracy_score(new_test, new_unigram_pred_int)} \\n f1_score:{f1_score(new_test, new_unigram_pred_int)}\")\n",
    "\n",
    "new_naive_mn_pred = tfidf_bigram_naive_MN.predict_proba(df1_ngram)\n",
    "new_naive_mn_pred_int = new_naive_mn_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Multinomial \\n Accuracy score: {accuracy_score(new_test, new_naive_mn_pred_int)} \\n f1_score:{f1_score(new_test, new_naive_mn_pred_int)}\")    \n",
    "\n",
    "\n",
    "new_naive_c_pred = tfidf_bigram_naive_C.predict_proba(df1_ngram)\n",
    "new_naive_c_pred_int = new_naive_c_pred[:,1] >= 0.5\n",
    "\n",
    "print(f\"Naive Bayes Complement \\n Accuracy score: {accuracy_score(new_test, new_naive_c_pred_int)} \\n f1_score:{f1_score(new_test, new_naive_c_pred_int)}\")     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4eebf-1237-41e4-acda-0624e5883588",
   "metadata": {},
   "source": [
    "## We make the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e606f-1949-4e6f-8958-518245765aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "new_predictions = {\"Bow\":new_lreg_bow_pred_int,\"Unigram\":new_unigram_pred_int, \"Naive_mn\":new_naive_mn_pred_int, \"Niave_c\":new_naive_c_pred_int}\n",
    "    \n",
    "for i, (name, pred) in enumerate(new_predictions.items()):\n",
    "    cm = confusion_matrix(new_test, pred)\n",
    "    \n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], [\"Pred N\", \"Pred P\"])\n",
    "    plt.yticks([0, 1], [\"N\", \"P\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"true Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8442fde-63f9-4761-a2f5-ca035c5edf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
