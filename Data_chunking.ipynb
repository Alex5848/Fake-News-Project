{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef65448f-f656-4cd8-9686-cbe026ea15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 19:27:43.760406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cleantext import clean\n",
    "import re\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import joblib \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39c314b-3b83-41cf-ad4e-6ede75a84c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv(\"news_cleaned_2018_02_13.csv\", chunksize=3000,usecols=[\"type\", \"content\"],skiprows=lambda x: x in range(1, 100000),lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406449b3-4781-49d1-8594-2da6c4c5ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(df:pd.DataFrame) -> (pd.DataFrame):\n",
    "    \"\"\"\n",
    "        Takes a TextFileReader with dataframes, and cleans the content. \n",
    "        \n",
    "        Returns the new TextFileReader with cleaned content, and a concatenated string of all the contents. \n",
    "    \"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    for text in df[\"content\"]:\n",
    "        if type(text) == str:\n",
    "            date_reg = re.compile(\"\\d{0,4}-\\d{0,2}-\\d{0,2}[^,]+\") # Cleaning the Dates of the text\n",
    "            date_subbed = date_reg.sub(\"<DAT>\", text)\n",
    "            date_reg2 = re.compile(\"/^(?:\\d{4})-(?:\\d{2})-(?:\\d{2})T(?:\\d{2}):(?:\\d{2}):(?:\\d{2}(?:\\.\\d*)?)(?:(?:-(?:\\d{2}):(?:\\d{2})|Z)?)$/\")\n",
    "            date_subbed2 = date_reg2.sub(\"<DAT>\", date_subbed)\n",
    "            cleaned_news_file = clean(date_subbed2,no_line_breaks=True, # Cleaning the rest of the text.\n",
    "                    no_urls=True,                  \n",
    "                    no_emails=True,                   \n",
    "                    no_numbers=True,\n",
    "                    no_punct=False,\n",
    "                    replace_with_number = \"<NUM>\")\n",
    "            df.loc[i,\"content\"] = cleaned_news_file # returning the cleaned text to the right position in our dataframe\n",
    "               \n",
    "        else:\n",
    "            pass \n",
    "        i += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7610cea-be25-4b2a-a6c8-78fe4bd5770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Takes a string of text and tokenizes it. \n",
    "    \n",
    "    Returns a list of the tokenized text. \n",
    "    \n",
    "    \"\"\"\n",
    "    token_list = text_to_word_sequence(text)\n",
    "    words = [word for word in token_list if word.isalpha()]\n",
    "    return words \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bcec4d2-cd8b-405d-8527-887a9bf776c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stopwords(token_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Takes a list of words and removes the stopwords \n",
    "    \n",
    "    Returns a list of words\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in token_list if not w in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fbc156f-275f-444c-98e8-e2fc9bcefd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(token_list:list) -> list:\n",
    "    \"\"\"\n",
    "    Takes a list of words and stems the words\n",
    "    \n",
    "    Returns a list of stemmed words. \n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in token_list]\n",
    "    return stemmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8392633b-0ab5-4000-b69c-dcea35bcd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['rumor', 'hate', 'unreliable', 'conspiracy', 'clickbait', 'satire',\n",
    "       'fake', 'reliable', 'bias', 'political', 'junksci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18965b08-cdd1-40af-a462-c145a699007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df):\n",
    "    df = df[df[\"type\"].isin(a)]\n",
    "    df = df.drop_duplicates(\"content\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5655ad77-a668-4be6-9796-eb9bd729e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_labels(df):\n",
    "    df_labels = df[\"type\"].isin([\"political\", \"reliable\" , \"clickbait\"])\n",
    "    df.insert(loc = len(df.columns) , column = \"label\" ,value = df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86664a98-93e7-4eb0-be4c-5ec65a55b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(df):\n",
    "    df[\"content\"] = df[\"content\"].apply(tokenize)\n",
    "    df[\"content\"] = df[\"content\"].apply(removing_stopwords)\n",
    "    df[\"content\"] = df[\"content\"].apply(stemming)\n",
    "\n",
    "    df[\"content\"] = df[\"content\"].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e26337-41af-469f-8940-3a66056d6825",
   "metadata": {},
   "source": [
    "### Since we are using chunks for learning we have to use the hashing vectorizer. If we didn't our vocabulary would increase pr chunk, and our therefor our feature space would increase, which means our training wouldn't work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7de0b7-c456-44af-bb28-efc7266f976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18, ngram_range=(1,3), alternate_sign=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f795f287-7da5-42ce-a29b-886271732b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_cls = SGDClassifier(loss=\"hinge\",random_state = 0)\n",
    "MNnb = MultinomialNB(fit_prior=False)\n",
    "Cnb = ComplementNB(fit_prior=False)\n",
    "MNnb_fp = MultinomialNB()\n",
    "Cnb_fp = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24693603-2efc-4d86-95dd-fcff5eb86f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"news_cleaned_2018_02_13.csv\", nrows=5000, skiprows=lambda x: x in range(1, 10000),usecols=[\"type\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d714ae7b-a2c0-45ce-97bc-a762444d970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = drop_rows(df2)\n",
    "df2 = clean_file(df2)\n",
    "insert_labels(df2)\n",
    "final_clean(df2)\n",
    "\n",
    "classes = df2[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d484a56-577c-4321-a01a-3da197624813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.fit_transform(df2[\"content\"])\n",
    "y_test = df2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f516b61-e4ed-4a45-bd0b-2bab4539ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_analyzed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19850dd-e76d-4135-96e9-4c2fe7e38519",
   "metadata": {},
   "source": [
    "### Main loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1e545-0ee5-4e9f-bd13-fecf2ce78032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 SGD Accuracy: 0.3104707792207792 f1_score: 0.45246535610699323\n",
      "iteration:0 MMnb Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:0 Cnb Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:0 Cnb_fp Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:0 MNnb_fp Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:10 SGD Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:10 MMnb Accuracy: 0.29910714285714285 f1_score: 0.4584509250548761\n",
      "iteration:10 Cnb Accuracy: 0.29910714285714285 f1_score: 0.4584509250548761\n",
      "iteration:10 Cnb_fp Accuracy: 0.29910714285714285 f1_score: 0.4584509250548761\n",
      "iteration:10 MNnb_fp Accuracy: 0.296875 f1_score: 0.4578313253012048\n",
      "iteration:20 SGD Accuracy: 0.3096590909090909 f1_score: 0.4532304725168756\n",
      "iteration:20 MMnb Accuracy: 0.36363636363636365 f1_score: 0.45706371191135736\n",
      "iteration:20 Cnb Accuracy: 0.36363636363636365 f1_score: 0.45706371191135736\n",
      "iteration:20 Cnb_fp Accuracy: 0.36363636363636365 f1_score: 0.45706371191135736\n",
      "iteration:20 MNnb_fp Accuracy: 0.3039772727272727 f1_score: 0.4572784810126582\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    if i > 720:\n",
    "        break \n",
    "        \n",
    "    df = clean_file(df)\n",
    "    df = drop_rows(df)\n",
    "    insert_labels(df)\n",
    "    final_clean(df)\n",
    "    \n",
    "    \n",
    "    if len(df[\"content\"]) > 100:\n",
    "        X_train = vectorizer.fit_transform(df[\"content\"])\n",
    "        SGD_cls.partial_fit(X_train, df[\"label\"], classes=classes)\n",
    "        \n",
    "        MNnb.partial_fit(X_train, df[\"label\"], classes=classes)\n",
    "        \n",
    "        Cnb.partial_fit(X_train, df[\"label\"], classes=classes)\n",
    "        \n",
    "        MNnb_fp.partial_fit(X_train, df[\"label\"], classes=classes)\n",
    "        \n",
    "        Cnb_fp.partial_fit(X_train, df[\"label\"], classes=classes)\n",
    "        \n",
    "        total_rows_analyzed += len(df[\"content\"])\n",
    "    if i%10 == 0:     \n",
    "        sgd_int = SGD_cls.predict(X_test)\n",
    "        \n",
    "        print(f\"iteration:{i} SGD Accuracy: {accuracy_score(y_test, sgd_int)} f1_score: {f1_score(y_test, sgd_int)}\")\n",
    "        \n",
    "        proba_MNnb = MNnb.predict_proba(X_test)\n",
    "        MNnb_int = proba_MNnb[:,1] > 0.5\n",
    "        print(f\"iteration:{i} MMnb Accuracy: {accuracy_score(y_test, MNnb_int)} f1_score: {f1_score(y_test, MNnb_int)}\")\n",
    "        \n",
    "        proba_Cnb = Cnb.predict_proba(X_test)\n",
    "        Cnb_int = proba_Cnb[:,1] > 0.5\n",
    "        print(f\"iteration:{i} Cnb Accuracy: {accuracy_score(y_test, Cnb_int)} f1_score: {f1_score(y_test, Cnb_int)}\")\n",
    "        \n",
    "        proba_Cnb_fp = Cnb_fp.predict_proba(X_test)\n",
    "        Cnb_fp_int = proba_Cnb_fp[:,1] > 0.5\n",
    "        print(f\"iteration:{i} Cnb_fp Accuracy: {accuracy_score(y_test, Cnb_fp_int)} f1_score: {f1_score(y_test, Cnb_fp_int)}\")\n",
    "        \n",
    "        proba_MNnb_fp = MNnb_fp.predict_proba(X_test)\n",
    "        MNnb_fp_int = proba_MNnb_fp[:,1] > 0.5\n",
    "        print(f\"iteration:{i} MNnb_fp Accuracy: {accuracy_score(y_test, MNnb_fp_int)} f1_score: {f1_score(y_test, MNnb_fp_int)}\")\n",
    "    else:\n",
    "        pass \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bb03c-53a7-4342-9bac-f2baa2281f21",
   "metadata": {},
   "source": [
    "### Dumping the models to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8aa7b-1272-46bd-b292-eb6b5f9909b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dumpmp(SGD_cls, \"SGD Classifier\")\n",
    "joblib.dumpmp(MNnb, \"MNnb Classifier\")\n",
    "joblib.dumpmp(Cnb, \"Cnb Classifier\")\n",
    "joblib.dumpmp(MNnb_fp, \"Nnb_fp Classifier\")\n",
    "joblib.dumpmp(SGD_cls, \"Cnb_fp Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedbacd-0907-463b-9754-6c0048713524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179d903-dbc2-408a-b34d-1941ac88cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows_analyzed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
